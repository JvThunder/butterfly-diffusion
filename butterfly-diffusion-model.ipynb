{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install diffusers[training]==0.11.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-23T06:43:33.627031Z","iopub.execute_input":"2023-09-23T06:43:33.627489Z","iopub.status.idle":"2023-09-23T06:43:46.071220Z","shell.execute_reply.started":"2023-09-23T06:43:33.627454Z","shell.execute_reply":"2023-09-23T06:43:46.069969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass TrainingConfig:\n    image_size = 128  # the generated image resolution\n    train_batch_size = 16\n    eval_batch_size = 16  # how many images to sample during evaluation\n    num_epochs = 50\n    learning_rate = 1e-4\n    lr_warmup_steps = 500\n    save_image_epochs = 10\n    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n    output_dir = 'pokemon-model'  # the model namy locally and on the HF Hub\n    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n    gradient_accumulation_steps = 1\n    seed = 0\n\nconfig = TrainingConfig()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:46.073791Z","iopub.execute_input":"2023-09-23T06:43:46.074501Z","iopub.status.idle":"2023-09-23T06:43:46.081796Z","shell.execute_reply.started":"2023-09-23T06:43:46.074463Z","shell.execute_reply":"2023-09-23T06:43:46.080850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nfrom PIL import Image\n\nimage_dir = '/kaggle/input/pokemon-red-gray/gray'\n\nclass PokeDataset(Dataset):\n    def __init__(self, image_dir, preprocess):\n        file_list = os.listdir(image_dir)\n        self.image_files = [os.path.join(image_dir, file) for file in file_list]\n        self.preprocess = preprocess\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        image = Image.open(self.image_files[idx]).convert('RGB')\n        return preprocess(image)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:46.083508Z","iopub.execute_input":"2023-09-23T06:43:46.083830Z","iopub.status.idle":"2023-09-23T06:43:46.099962Z","shell.execute_reply.started":"2023-09-23T06:43:46.083799Z","shell.execute_reply":"2023-09-23T06:43:46.098829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\n\npreprocess = transforms.Compose(\n    [\n        transforms.Resize((config.image_size, config.image_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:46.103366Z","iopub.execute_input":"2023-09-23T06:43:46.103983Z","iopub.status.idle":"2023-09-23T06:43:46.116856Z","shell.execute_reply.started":"2023-09-23T06:43:46.103949Z","shell.execute_reply":"2023-09-23T06:43:46.115746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = PokeDataset(image_dir, preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:46.119429Z","iopub.execute_input":"2023-09-23T06:43:46.120059Z","iopub.status.idle":"2023-09-23T06:43:46.128455Z","shell.execute_reply.started":"2023-09-23T06:43:46.120024Z","shell.execute_reply":"2023-09-23T06:43:46.127474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nconfig.dataset_name = \"huggan/smithsonian_butterflies_subset\"\ndataset = load_dataset(config.dataset_name, split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:46.129739Z","iopub.execute_input":"2023-09-23T06:43:46.130415Z","iopub.status.idle":"2023-09-23T06:43:47.664966Z","shell.execute_reply.started":"2023-09-23T06:43:46.130374Z","shell.execute_reply":"2023-09-23T06:43:47.663999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform(examples):\n    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return {\"images\": images}\n\ndataset.set_transform(transform)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:47.666466Z","iopub.execute_input":"2023-09-23T06:43:47.666797Z","iopub.status.idle":"2023-09-23T06:43:47.675337Z","shell.execute_reply.started":"2023-09-23T06:43:47.666763Z","shell.execute_reply":"2023-09-23T06:43:47.674379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0]['images']","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:47.676843Z","iopub.execute_input":"2023-09-23T06:43:47.677238Z","iopub.status.idle":"2023-09-23T06:43:47.701527Z","shell.execute_reply.started":"2023-09-23T06:43:47.677205Z","shell.execute_reply":"2023-09-23T06:43:47.700673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef show_image(image):\n    plt.imshow(np.transpose(image,(1,2,0)))\n    plt.axis('off')\n    plt.show()\n    \nshow_image(dataset[5]['images'])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:47.702929Z","iopub.execute_input":"2023-09-23T06:43:47.703250Z","iopub.status.idle":"2023-09-23T06:43:47.843010Z","shell.execute_reply.started":"2023-09-23T06:43:47.703215Z","shell.execute_reply":"2023-09-23T06:43:47.841789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_loader = DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:47.850740Z","iopub.execute_input":"2023-09-23T06:43:47.855203Z","iopub.status.idle":"2023-09-23T06:43:47.863556Z","shell.execute_reply.started":"2023-09-23T06:43:47.855152Z","shell.execute_reply":"2023-09-23T06:43:47.862425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from diffusers import UNet2DModel\n\nmodel = UNet2DModel(\n    sample_size=config.image_size,  # the target image resolution\n    in_channels=3,  # the number of input channels, 3 for RGB images\n    out_channels=3,  # the number of output channels\n    layers_per_block=2,  # how many ResNet layers to use per UNet block\n    block_out_channels=(\n        128, 128, 256, 256, 512, 512\n    ),  # the number of output channels for each UNet block\n    down_block_types=(\n        \"DownBlock2D\",  # a regular ResNet downsampling block\n        \"DownBlock2D\",\n        \"DownBlock2D\",\n        \"DownBlock2D\",\n        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n        \"DownBlock2D\",\n    ),\n    up_block_types=(\n        \"UpBlock2D\",  # a regular ResNet upsampling block\n        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:47.868857Z","iopub.execute_input":"2023-09-23T06:43:47.872178Z","iopub.status.idle":"2023-09-23T06:43:48.826576Z","shell.execute_reply.started":"2023-09-23T06:43:47.872130Z","shell.execute_reply":"2023-09-23T06:43:48.825602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from diffusers import DDPMScheduler\nnoise_scheduler = DDPMScheduler(num_train_timesteps=750)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:48.828141Z","iopub.execute_input":"2023-09-23T06:43:48.828563Z","iopub.status.idle":"2023-09-23T06:43:48.834265Z","shell.execute_reply.started":"2023-09-23T06:43:48.828526Z","shell.execute_reply":"2023-09-23T06:43:48.833155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_image = dataset[0]['images'].unsqueeze(0)\nprint('Input shape:', sample_image.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:48.835784Z","iopub.execute_input":"2023-09-23T06:43:48.836468Z","iopub.status.idle":"2023-09-23T06:43:48.855040Z","shell.execute_reply.started":"2023-09-23T06:43:48.836436Z","shell.execute_reply":"2023-09-23T06:43:48.854044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\noptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:48.856358Z","iopub.execute_input":"2023-09-23T06:43:48.856893Z","iopub.status.idle":"2023-09-23T06:43:48.864854Z","shell.execute_reply.started":"2023-09-23T06:43:48.856862Z","shell.execute_reply":"2023-09-23T06:43:48.863714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from diffusers.optimization import get_cosine_schedule_with_warmup\n\nlr_scheduler = get_cosine_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=config.lr_warmup_steps,\n    num_training_steps=(len(train_loader) * config.num_epochs),\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:48.867329Z","iopub.execute_input":"2023-09-23T06:43:48.867785Z","iopub.status.idle":"2023-09-23T06:43:48.876927Z","shell.execute_reply.started":"2023-09-23T06:43:48.867748Z","shell.execute_reply":"2023-09-23T06:43:48.876026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from diffusers import DDPMPipeline\n\nimport math\n\ndef make_grid(images, rows, cols):\n    w, h = images[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, image in enumerate(images):\n        grid.paste(image, box=(i%cols*w, i//cols*h))\n    return grid\n\ndef evaluate(config, epoch, pipeline):\n    # Sample some images from random noise (this is the backward diffusion process).\n    # The default pipeline output type is `List[PIL.Image]`\n    images = pipeline(\n        batch_size = config.eval_batch_size, \n        generator=torch.manual_seed(config.seed),\n    ).images\n\n    # Make a grid out of the images\n    image_grid = make_grid(images, rows=4, cols=4)\n\n    # Save the images\n    test_dir = os.path.join(config.output_dir, \"samples\")\n    os.makedirs(test_dir, exist_ok=True)\n    image_grid.save(f\"{test_dir}/{epoch+1}.png\")\n    image = Image.open(f\"{test_dir}/{epoch+1}.png\")\n    show_image(np.transpose(image,(2,1,0)))","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:48.878417Z","iopub.execute_input":"2023-09-23T06:43:48.878918Z","iopub.status.idle":"2023-09-23T06:43:48.890298Z","shell.execute_reply.started":"2023-09-23T06:43:48.878876Z","shell.execute_reply":"2023-09-23T06:43:48.889328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from accelerate import Accelerator\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport os\nimport torch.nn.functional as F\n\ndef train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n    # Initialize accelerator and tensorboard logging\n    accelerator = Accelerator(\n        mixed_precision=config.mixed_precision,\n        gradient_accumulation_steps=config.gradient_accumulation_steps, \n    )\n    \n    if accelerator.is_main_process:\n        if config.output_dir is not None:\n            os.makedirs(config.output_dir, exist_ok=True)\n        accelerator.init_trackers(\"train_example\")\n    \n    # Prepare everything\n    # There is no specific order to remember, you just need to unpack the \n    # objects in the same order you gave them to the prepare method.\n    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, lr_scheduler\n    )\n    \n    global_step = 0\n\n    # Now you train the model\n    for epoch in range(config.num_epochs):\n        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n        progress_bar.set_description(f\"Epoch {epoch}\")\n\n        for step, batch in enumerate(train_dataloader):\n            clean_images = batch['images']\n            # Sample noise to add to the images\n            noise = torch.randn(clean_images.shape).to(clean_images.device)\n            bs = clean_images.shape[0]\n\n            # Sample a random timestep for each image\n            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n\n            # Add noise to the clean images according to the noise magnitude at each timestep\n            # (this is the forward diffusion process)\n            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n            \n            with accelerator.accumulate(model):\n                # Predict the noise residual\n                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n                loss = F.mse_loss(noise_pred, noise)\n                accelerator.backward(loss)\n\n                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            \n            progress_bar.update(1)\n            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n            progress_bar.set_postfix(**logs)\n            accelerator.log(logs, step=global_step)\n            global_step += 1\n\n        # After each epoch you optionally sample some demo images with evaluate() and save the model\n        if accelerator.is_main_process:\n            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n\n            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n                evaluate(config, epoch, pipeline)\n                \n    pipeline.save_pretrained(config.output_dir) ","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:48.893051Z","iopub.execute_input":"2023-09-23T06:43:48.893340Z","iopub.status.idle":"2023-09-23T06:43:48.909272Z","shell.execute_reply.started":"2023-09-23T06:43:48.893294Z","shell.execute_reply":"2023-09-23T06:43:48.908225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loop(config, model, noise_scheduler, optimizer, train_loader, lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:43:48.911398Z","iopub.execute_input":"2023-09-23T06:43:48.912930Z","iopub.status.idle":"2023-09-23T06:44:44.218643Z","shell.execute_reply.started":"2023-09-23T06:43:48.912876Z","shell.execute_reply":"2023-09-23T06:44:44.216541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}